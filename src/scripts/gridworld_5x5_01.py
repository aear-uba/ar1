# -*- coding: utf-8 -*-
"""gridworld_5x5_01.ipynb

Automatically generated by Colab.



# =====================================================================
# Algoritmo de Evaluación de Política
#
# =====================================================================
# _Aprendizaje por Refuerzo I
# _Maestria en Inteligencia Artificial
# _UBA
# _2025
# =====================================================================

import numpy as np

'''

GridWorld de 5x5 (pseudocodigo)
 _________
|_|_|_|_|_|
|_|_|_|_|_|
|_|_|_|_|_|
|_|_|_|_|_|
|_|_|_|_|_|

=====================================================================
Entrada: π (la política a ser evaluada)
Inicializar un umbral θ > 0 (determina la precisión de la estimación)
Inicializar V(s) ← 0 para todo estado s

Repetir:
    Δ ← 0
    Para cada estado s en S:
        v ← V(s)
        V(s) ← Σ π(a | s) Σ p(s', r | s, a) [r + γ V(s')]
        Δ ← max(Δ, |v - V(s)|)
hasta que Δ < θ
=====================================================================

'''

# parametros
gamma = 0.9  # factor de descuento
theta = 1e-4  # umbral de convergencia
grid_size = 5  # tamanio del Gridworld
actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # Este, Oeste, Norte, Sur

# estados especiales con transiciones forzadas
special_states = {
    (0, 1): ((4, 1), 10),  # Estado A va a A' con +10
    (0, 3): ((2, 3), 5),   # Estado B va a B' con +5
}

V = np.zeros((grid_size, grid_size)) # inicializacion del valor del estado en cero

# iteraciones hasta convergencia
while True:
    delta = 0 # diferencia máxima entre iteraciones, para evaluar convergencia
    new_V = np.copy(V) # copia del valor del estado (en cero al inicio)

    for i in range(grid_size):
        for j in range(grid_size):
            # si el estado es especial, aplicar su transición forzada a A' o B'
            if (i, j) in special_states:
                new_V[i, j] = special_states[(i, j)][1] + gamma * V[special_states[(i, j)][0]]
                continue

            v = 0
            for action in actions:
                ni, nj = i + action[0], j + action[1]
                if 0 <= ni < grid_size and 0 <= nj < grid_size:
                    reward = 0 # movimientos normales sin recompensa
                    v += 0.25 * (reward + gamma * V[ni, nj])
                else:
                    v += 0.25 * (-1 + gamma * V[i, j])  # penalizacion de -1 al chocar con el borde

            new_V[i, j] = v # actualizacion del valor del estado
            delta = max(delta, abs(V[i, j] - new_V[i, j])) # calculo del cambio máximo entre iteraciones

    V = new_V
    if delta < theta:
        break

print("Gridworld Value Function:")
print(V)